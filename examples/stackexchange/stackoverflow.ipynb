{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training larger model\n",
    "In this example we'll train a recommender with more data. Specifically, we are going to be using the entire StackOverflow dataset, with over 600,000 users and 11M questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.datasets import fetch_stackexchange\n",
    "from lightfm.evaluation import auc_score\n",
    "\n",
    "# Downloading the data may take a short while\n",
    "data = fetch_stackexchange('stackoverflow',\n",
    "                           test_set_fraction=0.1,\n",
    "                          indicator_features=False,\n",
    "                          tag_features=True)\n",
    "\n",
    "train = data['train']\n",
    "test = data['test']\n",
    "features = data['item_features']\n",
    "labels = data['item_feature_labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 621,594 users and 11,280,896 items with 15,650,381 interactions.\n",
      "44,256 distinct tag features with labels like [u'c#', u'winforms', u'type-conversion'].\n"
     ]
    }
   ],
   "source": [
    "print('Training set: {:,} users and {:,} items with {:,} interactions.'\n",
    "     .format(train.shape[0], train.shape[1], train.getnnz()))\n",
    "print('{:,} distinct tag features with labels like {}.'\n",
    "     .format(features.shape[1], labels[:3].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the dataset is rather large, let's first run a single epoch and time it.\n",
    "\n",
    "The timings here are on a dual-core, relatively underpowered laptop: if you have a large multi-core server, the performance should be much better, and scale almost linearly with the number of cores you have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LightFM(loss='warp',\n",
    "               no_components=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 45s, sys: 280 ms, total: 14min 45s\n",
      "Wall time: 7min 26s\n"
     ]
    }
   ],
   "source": [
    "%time model = model.fit(train, item_features=features, epochs=1, num_threads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation can also be slow: to evaluate a learning-to-rank model, we need to compute full rankings for all users we have data for in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set has 159,551 users with test interactions.\n"
     ]
    }
   ],
   "source": [
    "print('Test set has {:,} users with test interactions.'.format((test.getnnz(axis=1) > 0).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit too large for efficient evaluation, as we will have to calculate model scores for all items for each user. To reduce this, let's randomly select a (much) smaller sample of users for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set has 151 users with test interactions.\n"
     ]
    }
   ],
   "source": [
    "test_small = (sp.diags([(np.random.random(size=test.shape[0]) < 0.001).astype(np.float32)], [0], format='csr')\n",
    "              * test.tocsr())\n",
    "print('Test set has {:,} users with test interactions.'.format((test_small.getnnz(axis=1) > 0).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 51s, sys: 48 ms, total: 5min 51s\n",
      "Wall time: 3min 29s\n"
     ]
    }
   ],
   "source": [
    "%time auc = auc_score(model, test_small, item_features=features, train_interactions=train, num_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
