{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommending questions on CrossValidated\n",
    "In this example, we'll try to recommend questions to be answered to users of stats.stackexchange.com.\n",
    "\n",
    "## Loading the data\n",
    "The full CrossValidated dataset is available at https://archive.org/details/stackexchange. Helper functions to obtain and process it are defined in `data.py`, and we are going to use them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import data\n",
    "\n",
    "(interactions, question_features,\n",
    " user_features, question_vectorizer,\n",
    " user_vectorizer) = data.read_data() # This will download the data if not present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`interactions` is a matrix with entries equal to 1 if the i-th user posted an answer to the j-th question; the goal is to recommend the questions to users who might answer them. `question_features` is a sparse matrix containing question metadata in the form of tags. `vectorizer` is a `sklearn.feature_extraction.DictVectorizer` instance that translates the tags into vector form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<61885x63570 sparse matrix of type '<type 'numpy.int32'>'\n",
      "\twith 63006 stored elements in Compressed Sparse Row format>\n",
      "<63570x1189 sparse matrix of type '<type 'numpy.int32'>'\n",
      "\twith 171861 stored elements in Compressed Sparse Row format>\n",
      "[{'bayesian': 1, 'elicitation': 1, 'prior': 1}, {'distributions': 1, 'normality': 1}, {'open-source': 1, 'software': 1}]\n",
      "[{'user_id:45': 1}]\n"
     ]
    }
   ],
   "source": [
    "print(repr(interactions))\n",
    "print(repr(question_features))\n",
    "print(question_vectorizer.inverse_transform(question_features[:3]))\n",
    "print(user_vectorizer.inverse_transform(user_features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split the dataset into train and test sets by using utility functions defined in `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def train_test_split(interactions):\n",
      "\n",
      "    train = interactions.copy()\n",
      "    test = interactions.copy()\n",
      "\n",
      "    for i in range(len(train.data)):\n",
      "        if random.random() < 0.2:\n",
      "            train.data[i] = 0\n",
      "        else:\n",
      "            test.data[i] = 0\n",
      "\n",
      "    train.eliminate_zeros()\n",
      "    test.eliminate_zeros()\n",
      "\n",
      "    return train, test\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import model\n",
    "import inspect\n",
    "print(inspect.getsource(model.train_test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = model.train_test_split(interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting models\n",
    "### Traditional MF model\n",
    "Let's start with a traditional collaborative filtering model that does not use any metadata. We can do this using `lightfm` -- we simply do not pass in any metadata matrices. We'll use the following function to train a WARP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def fit_lightfm_model(interactions, post_features):\n",
      "\n",
      "    model = lightfm.LightFM(loss='warp',\n",
      "                            no_components=30)\n",
      "\n",
      "    model.fit(interactions,\n",
      "              item_features=post_features,\n",
      "              epochs=10)\n",
      "\n",
      "    return model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(model.fit_lightfm_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mf_model = model.fit_lightfm_model(train, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will compute the AUC score on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def auc_lightfm(model, interactions, post_features):\n",
      "\n",
      "    no_users, no_items = interactions.shape\n",
      "\n",
      "    pid_array = np.arange(no_items, dtype=np.int32)\n",
      "\n",
      "    scores = []\n",
      "\n",
      "    for i in range(interactions.shape[0]):\n",
      "        uid_array = np.empty(no_items, dtype=np.int32)\n",
      "        uid_array.fill(i)\n",
      "        predictions = model.predict(uid_array,\n",
      "                                    pid_array,\n",
      "                                    item_features=post_features,\n",
      "                                    num_threads=2)\n",
      "        y = np.squeeze(np.array(interactions[i].todense()))\n",
      "\n",
      "        try:\n",
      "            scores.append(roc_auc_score(y, predictions))\n",
      "        except ValueError:\n",
      "            # Just one class\n",
      "            pass\n",
      "\n",
      "    return sum(scores) / len(scores)\n",
      "\n",
      "0.540323033962\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(model.auc_lightfm))\n",
    "mf_score = model.auc_lightfm(mf_model, test, None)\n",
    "print(mf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooops. That's barely better than random. In this case, this is because the CrossValidated dataset is very sparse: there just aren't enough interactions to support a traditional collaborative filtering model. In general, we'd also like to recommend questions that have no answers yet, making the collaborative model doubly ineffective.\n",
    "\n",
    "### Content-based model\n",
    "To remedy this, we can try using a content-based model. The following code uses question tags to estimate a logistic regression model for each user, predicting the probability that a user would want to answer a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def fit_content_models(interactions, post_features):\n",
      "\n",
      "    models = []\n",
      "\n",
      "    for user_row in interactions:\n",
      "        y = np.squeeze(np.array(user_row.todense()))\n",
      "\n",
      "        model = LogisticRegression(C=0.4)\n",
      "        try:\n",
      "            model.fit(post_features, y)\n",
      "        except ValueError:\n",
      "            # Just one class\n",
      "            pass\n",
      "\n",
      "        models.append(model)\n",
      "\n",
      "    return models\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(model.fit_content_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this and evaluating the AUC score gives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_models = model.fit_content_models(train, question_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.622322622991\n"
     ]
    }
   ],
   "source": [
    "content_score = model.auc_content_models(content_models, test, question_features)\n",
    "print(content_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a bit better, but not great.\n",
    "### Hybrid LightFM model\n",
    "What happens if we estimate theLightFM model _with_ question features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.646683848941\n"
     ]
    }
   ],
   "source": [
    "lightfm_model = model.fit_lightfm_model(train, post_features=question_features)\n",
    "lightfm_score = model.auc_lightfm(lightfm_model, test, post_features=question_features)\n",
    "print(lightfm_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82503800457\n"
     ]
    }
   ],
   "source": [
    "lightfm_model = model.fit_lightfm_model(train, post_features=question_features,\n",
    "                                         user_features=user_features)\n",
    "lightfm_score = model.auc_lightfm(lightfm_model, train, post_features=question_features,\n",
    "                                  user_features=user_features)\n",
    "print(lightfm_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<69776x85883 sparse matrix of type '<type 'numpy.int32'>'\\n\\twith 284212 stored elements in Compressed Sparse Row format>\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(user_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65162557761091566"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.auc_lightfm(lightfm_model, test, post_features=question_features, user_features=user_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
